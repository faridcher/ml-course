---
title: "Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance"
author: ""
date: "Dec 29, 2020"
output: 
    html_document:
        toc: true
        mathjax: local
        self_contained: FALSE
---

```{r opt, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache=TRUE, fig.align="center")
sources <- c("featureNormalize.r","plotFit.r","trainLinearReg.r",
             "linearRegCostFunction.r","polyFeatures.r","learningCurve.r",
             "validationCurve.r")
sources <- c(sources[1:3], paste0(substr(sources[4:7], 1, nchar(sources[4:7])-2), '-solution.r'))
invisible(lapply(sources, source))
```

This programming exercise instruction was originally developed and written by Prof. 
Andrew Ng as part of his machine learning [course](https://www.coursera.org/learn/machine-learning) on Coursera platform.
I have adapted the instruction for R language, so that its users, including myself, could also take and benefit from the course.

## Introduction

In this exercise, you will implement regularized linear regression and use it to
study models with different bias-variance properties. 
Before starting on the programming exercise, we strongly
recommend watching the video lectures and completing the review questions
for the associated topics.
To get started with the exercise, you will need to download the starter
code and unzip its contents to the directory where you wish to complete the
exercise. If needed, use the `setwd()` function in R to change to this directory before starting this exercise.

Files included in this exercise
 
- `ex5.r` - R script that steps you through the exercise
- `ex5data1.rda` - Dataset
- `submit.r` - Submission script that sends your solutions to our servers
- `featureNormalize.r` - Feature normalization function
- `plotFit.r` - Plot a polynomial fit
- `trainLinearReg.r` - Trains linear regression using your cost function
- [⋆] `linearRegCostFunction.r` - Regularized linear regression cost function
- [⋆] `learningCurve.r` - Generates a learning curve
- [⋆] `polyFeatures.r` - Maps data into polynomial feature space
- [⋆] `validationCurve.r` - Generates a cross validation curve

⋆ indicates files you will need to complete

Throughout the exercise, you will be using the script `ex5.r`. These scripts
set up the dataset for the problems and make calls to functions that you will
write. You are only required to modify functions in other files, by following
the instructions in this assignment.

### Where to get help

The exercises in this course use R, a high-level programming language
well-suited for numerical computations. If you do not have R installed, please
download a Windows installer from
[R-project](https://cran.r-project.org/bin/windows/base/) website.
[R-Studio](https://rstudio.com/products/rstudio/download/) is a free and
open-source R integrated development environment (IDE) making R script
development a bit easier when compared to the R's own basic GUI. You may start
from the `.rproj` (a R-Studio project file) in each exercise directory. 
At the R command line, typing help followed by a
function name displays documentation for that function. For example,
`help('plot')` or simply `?plot` will bring up help information for plotting.
Further documentation for R functions can be found at the R documentation pages. 

## 1 Regularized Linear Regression

In the first half of the exercise, you will implement regularized linear
regression to predict the amount of water flowing out of a dam using the change
of water level in a reservoir. In the next half, you will go through some
diagnostics of debugging learning algorithms and examine the effects of bias
v.s. variance. The provided script, `ex5.r`, will help you step through this exercise.

### 1.1 Visualizing the dataset

We will begin by visualizing the dataset containing historical records on the
change in the water level, $x$, and the amount of water flowing out of the dam, $y$.
This dataset is divided into three parts:

- A **training** set that your model will learn on: `X, y`
- A **cross validation** set for determining the regularization parameter: `Xval, yval`
- A **test** set for evaluating performance. These are "unseen" examples which
  your model did not see during training: `Xtest, ytest`

The next step of `ex5.r` will plot the training data (Figure 1). In the
following parts, you will implement linear regression and use that to fit a
straight line to the data and plot learning curves. Following that, you will
implement polynomial regression to find a better fit to the data.

```{r, fig.cap="Figure 1: Data", results="hide", echo=FALSE}
load(file="ex5data1.rda")
m <- dim(X)[1]
par(mar=c(3,3,.1,.1), mgp=c(1.5,.5,0))
plot(X, y, pch=4, lwd=2, col=2, xlab="change in water level (X)", 
     ylab= "water flowing out of the dam (y)")
```

### 1.2 Regularized linear regression cost function

Recall that regularized linear regression has the following cost function:

$$J(\theta)=\frac{1}{2m}(\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2)+\frac{\lambda}{2m}(\sum_{j=1}^n \theta_j^2),$$

where $\lambda$ is a regularization parameter which controls the degree of
regularization (thus, help preventing overfitting). The regularization term
puts a penalty on the overal cost $J$. As the magnitudes of the model parameters $\theta_j$
increase, the penalty increases as well. Note that you should not regularize
the $\theta_0$ term. (In R, the $\theta_0$ term is represented as `theta[1]`
since indexing in R starts from 1).
You should now complete the code in the file `linearRegCostFunction.r`.
Your task is to write a function to calculate the regularized linear regression
cost function. If possible, try to vectorize your code and avoid writing loops.
When you are finished, the next part of `ex5.r` will run your cost function
using theta initialized at `c(1,1)`. You should expect to see an output of 303.993.

*You should now submit your solutions.*

### 1.3 Regularized linear regression gradient

Correspondingly, the partial derivative of regularized linear regression's cost for $\theta_j$ is defined as

$$
\begin{array}{*2{>{\displaystyle}l}}
\frac{\partial J(\theta)}{\partial\theta_0}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} & \text{for $i=0$} \\
\frac{\partial J(\theta)}{\partial\theta_j}=\left(\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\right)+\frac{\lambda}{m}\theta_j & \text{for $j \geq 1$}
\end{array}
$$

In `linearRegCostFunction.r`, add code to calculate the gradient, returning it
in the variable grad. When you are finished, the next part of
`ex5.r` will run your gradient function using theta initialized at `c(1,1)`.
You should expect to see a gradient of `c(-15.30, 598.250)`.

*You should now submit your solutions.*

### 1.4 Fitting linear regression

Once your cost function and gradient are working correctly, the next part of
`ex5.r` will run the code in `trainLinearReg.r` to compute the optimal values
of $\theta$. This training function uses `optim` to optimize the cost function.
In this part, we set regularization parameter $\lambda$ to zero.
Because our current implementation of linear regression is trying to fit a 2-dimensional $\theta$,
regularization will not be incredibly helpful for a $\theta$ of such low dimension. In
the later parts of the exercise, you will be using polynomial regression with regularization.
Finally, the `ex5.r` script should also plot the best fit line, resulting in
an image similar to Figure 2. The best fit line tells us that the model is
not a good fit to the data because the data has a non-linear pattern. While
visualizing the best fit as shown is one possible way to debug your learning
algorithm, it is not always easy to visualize the data and model. In the next
section, you will implement a function to generate learning curves that can
help you debug your learning algorithm even if it is not easy to visualize the
data. 

```{r, fig.cap="Figure 2: Linear Fit", echo=FALSE}
theta <- c(1,1)
J <- linearRegCostFunction(cbind(1,X), y, 1)(theta)
grad <- linearRegGradFunction(cbind(1,X), y, 1)(theta)
lambda <- 0
theta <- trainLinearReg(cbind(1,X), y, lambda)
par(mar=c(3,3,.1,.1), mgp=c(1.5,.5,0))
plot(X, y, col="red", lwd=2, pch=4, 
     xlab="change in water level (x)",
     ylab="water flowing out of the dam (y)")
lines(X, cbind(1,X) %*% theta, lwd=2, col="blue")
```

## 2 Bias-variance

An important concept in machine learning is the bias-variance tradeoff. 
Models with high bias are not complex enough for the data and tend to underfit,
while models with high variance overfit to the training data.

In this part of the exercise, you will plot training and test errors on a
learning curve to diagnose bias-variance problems.

### 2.1 Learning curves

You will now implement code to generate the learning curves that will be
useful in debugging learning algorithms. Recall that a learning curve plots
training and cross validation error as a function of training set size. Your
job is to fill in `learningCurve.r` so that it returns a vector of errors for the
training set and cross validation set.
To plot the learning curve, we need a training and cross validation set
error for different training set sizes. To obtain different training set sizes,
you should use different subsets of the original training set $X$. Specifically, for
a training set size of i, you should use the first i examples (i.e., `X[1:i,]` and `y[1:i]`).
You can use the `trainLinearReg` function to find the $\theta$ parameters. Note
that the `lambda` is passed as a parameter to the `learningCurve` function.
After learning the $\theta$ parameters, you should compute the error on the
training and cross validation sets. Recall that the training error for a dataset is defined as

$$J_{train}(\theta)=\frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\right].$$

In particular, note that the training error does not include the regularization 
term. One way to compute the training error is to use your existing
cost function and set $\lambda$ to 0 only when using it to compute the training error
and cross validation error. When you are computing the training set error,
make sure you compute it on the training subset (i.e., `X[1:n,]` and `y[1:n]`)
(instead of the entire training set). However, for the cross validation error,
you should compute it over the entire cross validation set. You should store
the computed errors in the vectors error train and error val.
When you are finished, `ex5.r` wil print the learning curves and produce
a plot similar to Figure 3.

*You should now submit your solutions.*

In Figure 3, you can observe that both the train error and cross validation
error are high when the number of training examples is increased.
This reflects a **high bias** problem in the model – the linear regression model is
too simple and is unable to fit our dataset well. In the next section, you will
implement polynomial regression to fit a better model for this dataset.

```{r, fig.cap="Figure 3: Linear regression learning curve", echo=F}
lambda <- 0
lC <- learningCurve(cbind(1,X), y, cbind(1, Xval), yval, lambda)
error_train <- lC$error_train
error_val <- lC$error_val
rm(lC)
par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plot(c(1:m,1:m),c(error_train,error_val), type="n",
     xlab="Number of training examples", ylab="Error")
mtext('Learning curve for linear regression')
lines(1:m, error_train, type="l",col="blue")
lines(1:m, error_val, type="l", col="green")
legend("topright",c("Train","Cross Validation"), 
       col=c("blue","green"), lty=1)
```

## 3 Polynomial regression

The problem with our linear model was that it was too simple for the data
and resulted in underfitting (high bias). In this part of the exercise, you will
address this problem by adding more features.
For use polynomial regression, our hypothesis has the form:

\begin{align*}
h_\theta(x) &=\theta_0+\theta_1*(\text{waterLevel})+\theta_2*(\text{waterLevel})^2+\ldots+\theta_p*(\text{waterLevel})^p\\
&=\theta_0+\theta_1x_1+\theta_2x_2+\ldots+\theta_px_p
\end{align*}

Notice that by defining $x_1=(waterLevel), x_2=(waterLevel)^2, \ldots, x_p=(waterLevel)^p$,
we obtain a linear regression model where the features are
the various powers of the original value (waterLevel).
Now, you will add more features using the higher powers of the existing
feature $x$ in the dataset. Your task in this part is to complete the code in
`polyFeatures.r` so that the function maps the original training set `X` of size
$m \times 1$ into its higher powers. Specifically, when a training set `X` of size $m\times1$
is passed into the function, the function should return a $m\times p$ matrix `X_poly`,
where column 1 holds the original values of `X`, column 2 holds the values of
`X^2`, column 3 holds the values of `X^3`, and so on. Note that you don't
have to account for the zero-eth power in this function.
Now you have a function that will map features to a higher dimension,
and Part 6 of `ex5.r` will apply it to the training set, the test set, and the
cross validation set (which you haven't used yet).

*You should now submit your solutions.*

### 3.1 Learning Polynomial Regression

After you have completed `polyFeatures.r`, the `ex5.r` script will proceed to
train polynomial regression using your linear regression cost function.
Keep in mind that even though we have polynomial terms in our feature
vector, we are still solving a linear regression optimization problem. The
polynomial terms have simply turned into features that we can use for linear
regression. We are using the same cost function and gradient that you wrote
for the earlier part of this exercise.
For this part of the exercise, you will be using a polynomial of degree 8.
It turns out that if we run the training directly on the projected data, will
not work well as the features would be badly scaled (e.g., an example with
$x = 40$ will now have a feature $x_8=408=6.5\times10^{12}$). Therefore, you will
need to use feature normalization.
Before learning the parameters $\theta$ for the polynomial regression, `ex5.r` will
first call `featureNormalize` and normalize the features of the training set,
storing the `mu`, `sigma` parameters separately. We have already implemented
this function for you and it is the same function from the first exercise.
After learning the parameters $\theta$, you should see two plots (Figure 4,5)
generated for polynomial regression with $\lambda = 0$.
From Figure 4, you should see that the polynomial fit is able to follow
the datapoints very well - thus, obtaining a low training error. However, the
polynomial fit is very complex and even drops off at the extremes. This is
an indicator that the polynomial regression model is overfitting the training
data and will not generalize well.
To better understand the problems with the unregularized ($\lambda = 0$) model,
you can see that the learning curve (Figure 5) shows the same effect where
the low training error is low, but the cross validation error is high. There
is a gap between the training and cross validation errors, indicating a high
variance problem.

```{r, fig.cap="Figure 4: Polynomial fit, $\lambda = 0$", echo=F}
p <- 8
X_poly <- polyFeatures(X, p)
fN <- featureNormalize(X_poly)  # Normalize
X_poly <- fN$X_norm
mu <- fN$mu
sigma <- fN$sigma
rm(fN)
X_poly <- cbind(rep(1,m), X_poly)
# Map X_poly_test and normalize (using mu and sigma)
X_poly_test <- polyFeatures(Xtest, p)
X_poly_test <- matrix(mapply(`-`,t(X_poly_test),mu),dim(X_poly_test) ,byrow = TRUE)
X_poly_test <- matrix(mapply(`/`,t(X_poly_test),sigma),dim(X_poly_test) ,byrow = TRUE)
X_poly_test <-  cbind(rep(1,dim(X_poly_test)[1]), X_poly_test)
# Map X_poly_val and normalize (using mu and sigma)
X_poly_val <- polyFeatures(Xval, p)
X_poly_val <- matrix(mapply(`-`,t(X_poly_val),mu),dim(X_poly_val) ,byrow = TRUE)
X_poly_val <- matrix(mapply(`/`,t(X_poly_val),sigma),dim(X_poly_val) ,byrow = TRUE)
X_poly_val <-  cbind(rep(1,dim(X_poly_val)[1]), X_poly_val)

lambda <- 0
theta <- trainLinearReg(X_poly, y, lambda)
par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plotFit(min(X), max(X), mu, sigma, theta, p)
points(X, y, col="red", lwd=2, pch=4)
```

```{r, fig.cap="Figure 5: Polynomial learning curve, $\lambda = 0$", echo=F}
lC <- learningCurve(X_poly, y, X_poly_val, yval, lambda)
error_train <- lC$error_train
error_val <- lC$error_val
rm(lC)

par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plot(c(1:m,1:m),c(error_train,error_val), type="n",
      xlab='Number of training examples', ylab="Error")
mtext(sprintf('Polynomial Regression \nLearning Curve (lambda=%3.3f)', lambda))
lines(1:m, error_train, type="l",col="blue",lwd=2)
lines(1:m, error_val, type="l", col="green", lwd=2)
legend("topright",c("Train","Cross Validation"), 
       col=c("blue","green"), lty=1)
```

One way to combat the overfitting (high-variance) problem is to add
regularization to the model. In the next section, you will get to try different $\lambda$ parameters to see how regularization can lead to a better model.

### 3.2 Optional (ungraded) exercise: Adjusting the regularization parameter

In this section, you will get to observe how the regularization parameter
affects the bias-variance of regularized polynomial regression. You should
now modify the lambda parameter in the `ex5.r` and try $\lambda = 1, 100$. For
each of these values, the script should generate a polynomial fit to the data
and also a learning curve.
For $\lambda = 1$, you should see a polynomial fit that follows the data trend
well (Figure 6) and a learning curve (Figure 7) showing that both the cross
validation and training error converge to a relatively low value. This shows
the $\lambda = 1$ regularized polynomial regression model does not have the high bias or high-variance problems. In effect, it achieves a good trade-off between bias and variance.
For $\lambda = 100$, you should see a polynomial fit (Figure 8) that does not
follow the data well. In this case, there is too much regularization and the
model is unable to fit the training data.

You do not need to submit any solutions for this optional (ungraded) exercise.

```{r, fig.cap="Figure 6: Polynomial fit, $\lambda = 1$", echo=F}
lambda <- 1
theta <- trainLinearReg(X_poly, y, lambda)
par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plotFit(min(X), max(X), mu, sigma, theta, p)
points(X, y, col="red", lwd=2, pch=4)
```

```{r, fig.cap="Figure 7: Polynomial learning curve, $\lambda = 1$", echo=F}
lC <- learningCurve(X_poly, y, X_poly_val, yval, lambda)
error_train <- lC$error_train
error_val <- lC$error_val
rm(lC)

par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plot(c(1:m,1:m),c(error_train,error_val), type="n",
      xlab='Number of training examples', ylab="Error")
mtext(sprintf('Polynomial Regression \nLearning Curve (lambda=%3.3f)', lambda))
lines(1:m, error_train, type="l",col="blue",lwd=2)
lines(1:m, error_val, type="l", col="green", lwd=2)
legend("topright",c("Train","Cross Validation"), 
       col=c("blue","green"), lty=1)
```

```{r, fig.cap="Figure 8: Polynomial fit, $\lambda = 100$", echo=F}
lambda <- 100
theta <- trainLinearReg(X_poly, y, lambda)
par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plotFit(min(X), max(X), mu, sigma, theta, p)
points(X, y, col="red", lwd=2, pch=4)
```

### 3.3 Selecting $\lambda$ using a cross validation set

From the previous parts of the exercise, you observed that the value of $\lambda$
can significantly affect the results of regularized polynomial regression on
the training and cross validation set. In particular, a model without regularization ($\lambda = 0$) fits the training set well, 
but does not generalize. Conversely, a model with too much regularization ($\lambda$ = 100) does not fit the training set
and testing set well. A good choice of $\lambda$ (e.g., $\lambda = 1$) can provide a good fit
to the data.
In this section, you will implement an automated method to select the $\lambda$ parameter.
Concretely, you will use a cross validation set to evaluate
how good each $\lambda$ value is. After selecting the best $\lambda$ value using the cross
validation set, we can then evaluate the model on the test set to estimate
how well the model will perform on actual unseen data.
Your task is to complete the code in `validationCurve.r`. Specifically,
you should should use the `trainLinearReg` function to train the model using
different values of $\lambda$ and compute the training error and cross validation error.
You should try $\lambda$ in the following range: ${0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}$.

```{r, fig.cap="Figure 9: Selecting $\lambda$ using a cross validation set", echo=F}
vC <- validationCurve(X_poly, y, X_poly_val, yval)
lambda_vec <- vC$lambda_vec
error_train <- vC$error_train
error_val <- vC$error_val
rm(vC)

par(mar=c(3,3,.1,.1), mgp=c(1.5,.5,0))
plot(c(lambda_vec,lambda_vec),c(error_train,error_val), type="n",
     xlab='lambda', ylab="Error")
lines(lambda_vec, error_train, type="l",col="blue",lwd=2)
lines(lambda_vec, error_val, type="l", col="green", lwd=2)
legend("topright",c("Train","Cross Validation"), 
       col=c("blue","green"), lty=1)
```

After you have completed the code, the next part of `ex5.r` will run your
function can plot a cross validation curve of error v.s. $\lambda$ that allows you select
which $\lambda$ parameter to use. You should see a plot similar to Figure 9. In this
figure, we can see that the best value of $\lambda$ is around 3. Due to randomness
in the training and validation splits of the dataset, the cross validation error
can sometimes be lower than the training error.

*You should now submit your solutions.*

### 3.4 Optional (ungraded) exercise: Computing test set error

In the previous part of the exercise, you implemented code to compute the
cross validation error for various values of the regularization parameter λ.
However, to get a better indication of the model's performance in the real
world, it is important to evaluate the "final" model on a test set that was
not used in any part of training (that is, it was neither used to select the $\lambda$
parameters, nor to learn the model parameters $\theta$).
For this optional (ungraded) exercise, you should compute the test error
using the best value of $\lambda$ you found. In our cross validation, we obtained a
test error of $3.8599$ for $\lambda=3$.

You do not need to submit any solutions for this optional (ungraded)
exercise.

### 3.5 Optional (ungraded) exercise: Plotting learning curves with randomly selected examples

In practice, especially for small training sets, when you plot learning curves
to debug your algorithms, it is often helpful to average across multiple sets
of randomly selected examples to determine the training error and cross
validation error.
Concretely, to determine the training error and cross validation error for
i examples, you should first randomly select i examples from the training set
and i examples from the cross validation set. You will then learn the
parameters $\theta$ using the randomly chosen training set and evaluate the
parameters
$\theta$ on the randomly chosen training set and cross validation set. The above
steps should then be repeated multiple times (say 50) and the averaged error
should be used to determine the training error and cross validation error for
i examples.
For this optional (ungraded) exercise, you should implement the above
strategy for computing the learning curves. For reference, figure 10 shows the
learning curve we obtained for polynomial regression with $\lambda = 0.01$. Your
figure may differ slightly due to the random selection of examples.
You do not need to submit any solutions for this optional (ungraded)
exercise.

```{r, fig.cap="Figure 10: Optional (ungraded) exercise: Learning curve with randomly selected examples", echo=F}
lambda <- 0.01
lC <- learningCurve(X_poly, y, X_poly_val, yval, lambda)
error_train <- lC$error_train
error_val <- lC$error_val
rm(lC)

par(mar=c(3,3,2,.1), mgp=c(1.5,.5,0))
plot(c(1:m,1:m), c(error_train,error_val), type="n",
     xlab='Number of training examples', ylab="Error")
mtext(sprintf('Polynomial Regression \nLearning Curve (lambda=%3.3f)', lambda))
lines(1:m, error_train, type="l",col="blue",lwd=2)
lines(1:m, error_val, type="l", col="green", lwd=2)
legend("topright",c("Train","Cross Validation"), 
       col=c("blue","green"), lty=1)
```

## Submission and Grading

After completing various parts of the assignment, be sure to use the submit
function system to submit your solutions to our servers. The following is a
breakdown of how each part of this exercise is scored.

| Part                                        | Submitted File            | Points     |
| :-------------                              | :----                     | :--------: |
| Regularized Linear Regression Cost Function | `linearRegCostFunction.r` | 25 points  |
| Regularized Linear Regression Gradient      | `linearRegCostFunction.r` | 25 points  |
| Learning Curve                              | `learningCurve.r`         | 20 points  |
| Polynomial Feature Mapping                  | `polyFeatures.r`          | 10 points  |
| Cross Validation Curve                      | `validationCurve.r`       | 20 points  |
| Total Points                                |                           | 100 points |

You are allowed to submit your solutions multiple times, and we will take
only the highest score into consideration.
